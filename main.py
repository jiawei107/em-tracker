"""
Journal Manuscript Tracker - ç®€åŒ–ç‰ˆ
ä»é…ç½®æ–‡ä»¶è¯»å–è´¦æˆ·ä¿¡æ¯å¹¶è¿½è¸ªç¨¿ä»¶çŠ¶æ€
"""

import requests
import re
import time
import logging
from datetime import datetime
from bs4 import BeautifulSoup
from typing import Union, Dict, List, Any
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode

# é…ç½®å¯¼å…¥
try:
    from config import (
        ACCOUNTS, BASE_URL, LOGIN_SUCCESS_FLAG, DEFAULT_RETRY_COUNT,
        RETRY_DELAY_SECONDS, DEFAULT_TIMEOUT, BROWSER_HEADERS,
        SERVERCHAN_SENDKEY, LOG_FILE, LOG_LEVEL
    )
except ImportError as e:
    print(f"[é”™è¯¯] æœªæ‰¾åˆ° config.py æ–‡ä»¶æˆ–é…ç½®ä¸å®Œæ•´: {e}")
    print("è¯·å…ˆå¤åˆ¶ config_template.py ä¸º config.py å¹¶å¡«å†™é…ç½®ä¿¡æ¯ã€‚")
    exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL.upper(), logging.INFO),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def send_to_serverchan(title: str, content: str) -> bool:
    """é€šè¿‡ Serveré…± API æ¨é€æ¶ˆæ¯åˆ°å¾®ä¿¡"""
    if not SERVERCHAN_SENDKEY or SERVERCHAN_SENDKEY == 'your_sendkey_here':
        logger.warning("Serveré…± SendKey æœªé…ç½®ï¼Œè·³è¿‡å¾®ä¿¡æ¨é€")
        return False

    url = f"https://sctapi.ftqq.com/{SERVERCHAN_SENDKEY}.send"
    data = {
        'title': title,
        'desp': content
    }

    try:
        response = requests.post(url, data=data, timeout=10)
        result = response.json()

        if result.get('code') == 0:
            logger.info("å¾®ä¿¡æ¨é€æˆåŠŸ")
            return True
        else:
            logger.error(f"å¾®ä¿¡æ¨é€å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return False

    except Exception as e:
        logger.error(f"å¾®ä¿¡æ¨é€å¼‚å¸¸: {e}")
        return False


def perform_login(account: dict) -> Union[requests.Session, None]:
    """æ‰§è¡Œç™»å½•æ“ä½œå¹¶è¿”å›å·²è®¤è¯çš„session"""
    journal_short = account['journal_short_name']
    login_url = f"{BASE_URL}/{journal_short}/LoginAction.ashx"
    login_payload = {'username': account['username'], 'password': account['password']}

    logger.info(f"æ­£åœ¨ç™»å½• '{account['journal_full_name']}'...")

    for attempt in range(DEFAULT_RETRY_COUNT):
        try:
            session = requests.Session()
            session.headers.update(BROWSER_HEADERS)

            response = session.post(login_url, data=login_payload, timeout=DEFAULT_TIMEOUT)
            response.raise_for_status()

            if LOGIN_SUCCESS_FLAG in response.text:
                logger.info(f"ç™»å½•æˆåŠŸï¼è´¦æˆ·: {account['username']}")
                return session
            else:
                logger.error(f"ç™»å½•å¤±è´¥ã€‚è¯·æ£€æŸ¥è´¦æˆ·ä¿¡æ¯: {account['username']}")
                return None

        except requests.exceptions.RequestException as e:
            logger.error(f"ç½‘ç»œé”™è¯¯: {e}")

        if attempt < DEFAULT_RETRY_COUNT - 1:
            logger.info(f"{RETRY_DELAY_SECONDS} ç§’åé‡è¯•...")
            time.sleep(RETRY_DELAY_SECONDS)

    logger.error(f"æ‰€æœ‰ç™»å½•å°è¯•å‡å¤±è´¥: {account['username']}")
    return None


def fetch_manuscript_details(session: requests.Session, detail_url: str, referer_url: str) -> List[Dict[str, Any]]:
    """è·å–ç¨¿ä»¶è¯¦ç»†ä¿¡æ¯"""
    page_name = detail_url.split('/')[-1].split('?')[0]

    try:
        headers = BROWSER_HEADERS.copy()
        headers['Referer'] = referer_url
        response = session.get(detail_url, headers=headers, timeout=DEFAULT_TIMEOUT)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')

        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é¡µå™¨
        if soup.find('select', {'name': 'size1'}):
            parsed_url = urlparse(detail_url)
            query_params = parse_qs(parsed_url.query)
            query_params['size1'] = ['500']
            query_params['size2'] = ['500']
            full_data_url = urlunparse(parsed_url._replace(query=urlencode(query_params, doseq=True)))
            response = session.get(full_data_url, headers=headers, timeout=DEFAULT_TIMEOUT)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')

        manuscript_list = []
        table = soup.find('table', id='datatable') or soup.find('table', id='searchresults')
        if not table:
            return []

        thead = table.find('thead')
        tbody = table.find('tbody')
        if not thead or not tbody:
            return []

        original_headers = [th.get_text(strip=True) for th in thead.find_all('th')]
        data_rows = tbody.find_all('tr')

        for row in data_rows:
            cells = [child for child in row.children if child.name == 'td']
            if len(cells) == len(original_headers):
                manuscript_data = {original_headers[i]: cells[i].get_text(strip=True) for i in range(len(cells))}

                # æå–docid
                action_link = cells[0].find('a', href=re.compile(r'docid=(\d+)'))
                if action_link:
                    match = re.search(r'docid=(\d+)', action_link['href'])
                    if match:
                        manuscript_data['docid'] = match.group(1)

                manuscript_list.append(manuscript_data)

        logger.debug(f"æˆåŠŸè·å– {len(manuscript_list)} æ¡ç¨¿ä»¶è®°å½•: {page_name}")
        return manuscript_list

    except Exception as e:
        logger.error(f"è·å–è¯¦æƒ…å¤±è´¥: {e}")
        return []


def fetch_submission_overview(session: requests.Session, account: dict) -> Union[List[Dict[str, Any]], None]:
    """è·å–æŠ•ç¨¿æ¦‚è§ˆ"""
    journal_short = account['journal_short_name']
    base_journal_url = f"{BASE_URL}/{journal_short}/"
    main_menu_url = f"{base_journal_url}AuthorMainMenu.aspx"

    logger.info("æ­£åœ¨è·å–ç¨¿ä»¶åˆ—è¡¨...")

    try:
        headers = BROWSER_HEADERS.copy()
        headers['Referer'] = f'{base_journal_url}default2.aspx'
        response = session.get(main_menu_url, headers=headers, timeout=DEFAULT_TIMEOUT)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        logger.error(f"è·å–ä¸»èœå•å¤±è´¥: {e}")
        return None

    try:
        soup = BeautifulSoup(response.text, 'html.parser')
        all_manuscripts = []

        for link in soup.select('fieldset.datatablecontainer div.main_menu_item a'):
            count_span = link.find_next_sibling('span', class_='count')
            if count_span and '(0)' not in count_span.get_text():
                full_detail_url = f"{base_journal_url}{link['href']}"
                details = fetch_manuscript_details(session, full_detail_url, main_menu_url)
                if details:
                    all_manuscripts.extend(details)

        logger.info(f"å…±å‘ç° {len(all_manuscripts)} æ¡ç¨¿ä»¶è®°å½•")
        return all_manuscripts

    except Exception as e:
        logger.error(f"è§£æå¤±è´¥: {e}")
        return None


def find_value_by_partial_key(data: Dict[str, str], key_parts: List[str]) -> str:
    """é€šè¿‡éƒ¨åˆ†å…³é”®è¯åŒ¹é…å­—å…¸ä¸­çš„å€¼"""
    for key, value in data.items():
        normalized_key = ''.join(key.split()).lower()
        if any(part.lower() in normalized_key for part in key_parts):
            return value
    return ''


def format_manuscript_info(manuscript: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–å•æ¡ç¨¿ä»¶ä¿¡æ¯"""
    title = find_value_by_partial_key(manuscript, ['title']) or 'No-Title-Found'
    ms_number = find_value_by_partial_key(manuscript, ['manuscriptnumber'])
    submission_date = find_value_by_partial_key(manuscript, ['submissionbegan', 'initialdate', 'datesubmitted'])
    status_date = find_value_by_partial_key(manuscript, ['statusdate'])
    status = find_value_by_partial_key(manuscript, ['currentstatus'])

    return (
        f"ğŸ“„ {title[:50]}...\n"
        f"   ç¨¿ä»¶ç¼–å·: {ms_number}\n"
        f"   å½“å‰çŠ¶æ€: {status}\n"
        f"   çŠ¶æ€æ—¥æœŸ: {status_date}\n"
        f"   æŠ•ç¨¿æ—¥æœŸ: {submission_date}\n"
    )


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("Journal Manuscript Tracker - ç®€åŒ–ç‰ˆ")
    logger.info("=" * 60)

    all_results = []
    wechat_content = ""

    # éå†æ‰€æœ‰è´¦æˆ·
    for account in ACCOUNTS:
        if 'your_username' in account.get('username', '').lower():
            logger.warning(f"è·³è¿‡æœªé…ç½®çš„è´¦æˆ·: {account.get('username', '')}")
            continue

        logger.info("=" * 60)
        logger.info(f"æœŸåˆŠ: {account['journal_full_name']} ({account['journal_short_name']})")
        logger.info(f"è´¦æˆ·: {account['username']}")
        logger.info("=" * 60)

        # ç™»å½•
        session = perform_login(account)
        if not session:
            continue

        # è·å–ç¨¿ä»¶ä¿¡æ¯
        manuscripts = fetch_submission_overview(session, account)
        if not manuscripts:
            logger.info("æ— ç¨¿ä»¶è®°å½•ã€‚")
            continue

        # å¤„ç†æ¯æ¡ç¨¿ä»¶è®°å½•
        wechat_content += f"\n### {account['journal_full_name']}\n\n"

        for manuscript in manuscripts:
            title = find_value_by_partial_key(manuscript, ['title']) or 'No-Title-Found'
            ms_number = find_value_by_partial_key(manuscript, ['manuscriptnumber'])
            submission_date = find_value_by_partial_key(manuscript, ['submissionbegan', 'initialdate', 'datesubmitted'])
            status_date = find_value_by_partial_key(manuscript, ['statusdate'])
            status = find_value_by_partial_key(manuscript, ['currentstatus'])

            result = {
                'æ—¶é—´æˆ³': datetime.now().strftime('%Y-%m-%d %H:%M'),
                'æŠ•ç¨¿æ—¥æœŸ': submission_date,
                'çŠ¶æ€æ—¥æœŸ': status_date,
                'å½“å‰çŠ¶æ€': status,
                'ç¨¿ä»¶ç¼–å·': ms_number
            }

            all_results.append(result)

            logger.info(f"ç¨¿ä»¶: {title[:50]}...")
            logger.info(f"  ç¼–å·: {ms_number}")
            logger.info(f"  çŠ¶æ€: {status}")
            logger.info(f"  çŠ¶æ€æ—¥æœŸ: {status_date}")

            # æ·»åŠ åˆ°å¾®ä¿¡æ¨é€å†…å®¹
            wechat_content += f"ğŸ“„ **{title[:40]}...**\n"
            wechat_content += f"â€¢ ç¼–å·: {ms_number}\n"
            wechat_content += f"â€¢ çŠ¶æ€: {status}\n"
            wechat_content += f"â€¢ æ—¥æœŸ: {status_date}\n\n"

    # è¾“å‡ºæ±‡æ€»ä¿¡æ¯åˆ°æ—¥å¿—
    if all_results:
        logger.info("=" * 60)
        logger.info("æ±‡æ€»ç»“æœ")
        logger.info("=" * 60)

        for result in all_results:
            logger.info(
                f"æ—¶é—´æˆ³: {result['æ—¶é—´æˆ³']} | "
                f"ç¨¿ä»¶ç¼–å·: {result['ç¨¿ä»¶ç¼–å·']} | "
                f"çŠ¶æ€: {result['å½“å‰çŠ¶æ€']} | "
                f"çŠ¶æ€æ—¥æœŸ: {result['çŠ¶æ€æ—¥æœŸ']}"
            )

        logger.info(f"å…±å¤„ç† {len(all_results)} æ¡ç¨¿ä»¶è®°å½•")

        # æ¨é€åˆ°å¾®ä¿¡
        wechat_title = f"ğŸ“Š ç¨¿ä»¶è¿½è¸ªç»“æœ ({datetime.now().strftime('%Y-%m-%d %H:%M')})"
        if wechat_content.strip():
            send_to_serverchan(wechat_title, wechat_content)
    else:
        logger.warning("æœªè·å–åˆ°ä»»ä½•ç¨¿ä»¶ä¿¡æ¯ã€‚")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
    except Exception as e:
        logger.error(f"æœªå¤„ç†çš„é”™è¯¯: {e}", exc_info=True)
